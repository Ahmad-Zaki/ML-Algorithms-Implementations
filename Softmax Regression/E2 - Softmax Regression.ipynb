{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement and train Softmax Regression with mini-batch SGD and early stopping.\n",
    "\n",
    "The expected outcome.\n",
    "* Implement Softmax Regression Model.\n",
    "* Implement mini-batch SGD.\n",
    "* The training should support early stopping.\n",
    "* Train and evaluate the model with cross-validation. The evaluation metric is the *accuracy*.\n",
    "* Retrain the model with early stopping.\n",
    "\n",
    "\n",
    "**DO NOT USE SKLEARN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
       "0                5.1               3.5                1.4               0.2   \n",
       "1                4.9               3.0                1.4               0.2   \n",
       "2                4.7               3.2                1.3               0.2   \n",
       "3                4.6               3.1                1.5               0.2   \n",
       "4                5.0               3.6                1.4               0.2   \n",
       "\n",
       "   target  \n",
       "0       0  \n",
       "1       0  \n",
       "2       0  \n",
       "3       0  \n",
       "4       0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris[\"data\"]\n",
    "y = iris[\"target\"]\n",
    "df = pd.DataFrame({fname: values for fname, values in zip(iris[\"feature_names\"], X.T)})\n",
    "df[\"target\"] = y\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your Code\n",
    "You can start writing your code from here. Please don't modify any of the previous code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class softmaxRegressor():\n",
    "    def __init__(self, lr=0.01, epochs=100, early_stopping=True):\n",
    "        import numpy as np\n",
    "        self.epochs = epochs\n",
    "        self.lr = lr\n",
    "        self.early_stopping = early_stopping\n",
    "        self.w = None\n",
    "        self.num_features = None     #Holds the number of features of training data to check the dimensions at prediction.\n",
    "        self.is_trained = False      #Flag to denote if the model has been trained\n",
    "    \n",
    "    def softmax(self, z):\n",
    "        exp = np.exp(z)  \n",
    "        for i in range(len(z)):\n",
    "            exp[i] /= np.sum(exp[i])\n",
    "        return exp\n",
    "    \n",
    "    def fit(self, X, y, batch_size=None):\n",
    "        if type(X) != np.ndarray:\n",
    "            X = X.to_numpy()\n",
    "            \n",
    "        m, n = X.shape    \n",
    "        x0 = np.ones((m, 1))        #Bias term.\n",
    "        X = np.hstack((x0, X))\n",
    "        \n",
    "        y_hot = pd.get_dummies(y)   #One hot encoder.\n",
    "        c = y_hot.shape[1]          #Number of classes.\n",
    "        \n",
    "        w = np.zeros((n+1, c))      #Weights initialization.\n",
    "        \n",
    "        loss_lst = []\n",
    "        #If batch size is not specified: operate on all the data\n",
    "        if batch_size == None: batch_size = m\n",
    "        \n",
    "        \n",
    "        for i in range(self.epochs):\n",
    "            for j in range(0, m, batch_size):\n",
    "                #Load next batch:\n",
    "                x_b = X[j:j+batch_size, :]\n",
    "                y_b = y.iloc[j:j+batch_size]\n",
    "                y_b_hot = y_hot.iloc[j:j+batch_size, :]\n",
    "                m = x_b.shape[0]\n",
    "                    \n",
    "                z = x_b @ w\n",
    "                y_hat = self.softmax(z)\n",
    "                \n",
    "                loss = -np.mean(np.log(y_hat[np.arange(len(y_b)), y_b]))\n",
    "                loss_lst.append(loss)\n",
    "                \n",
    "                dw = np.dot(x_b.T, (y_hat-y_b_hot))/m  #weights Gradient\n",
    "                w -= self.lr*dw                        #Weights update\n",
    "                \n",
    "            #Early stopping Condition:\n",
    "            if (self.early_stopping == True) and (abs(loss_lst[-1]-loss_lst[-2]) <=0.001):\n",
    "                print(f\"Training Stopped at the {i}th epoch\")\n",
    "                break\n",
    "        \n",
    "        \n",
    "        self.w = w\n",
    "        self.num_features = n\n",
    "        self.is_trained = True\n",
    "        return self\n",
    "        \n",
    "    def predict(self, X):\n",
    "        m, n = X.shape\n",
    "        #Check if the model is trained:\n",
    "        assert self.is_trained, \"Model is not trained\"\n",
    "        #Check input size:\n",
    "        assert n == self.num_features, f\"Input shape mismatch: expected {self.num_features} features but got {n}\"\n",
    "\n",
    "        x0 = np.ones((m, 1))    #Bias term.\n",
    "        X = np.hstack((x0, X))\n",
    "\n",
    "        z = X @ self.w\n",
    "        y_hat = self.softmax(z)\n",
    "\n",
    "        # Returning the class with highest probability.\n",
    "        return np.argmax(y_hat, axis=1)\n",
    "    \n",
    "    #Predict and Calculate accuracy\n",
    "    def score(self, X, y):\n",
    "        y_hat = self.predict(X)\n",
    "        return np.sum(y==y_hat)/len(y)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the following cell to train and evaluate your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Stopped at the 136th epoch\n",
      "Test set accuracy:\n",
      "0.9333333333333333\n",
      "Training Stopped at the 112th epoch\n",
      "Test set accuracy:\n",
      "0.9666666666666667\n",
      "Training Stopped at the 144th epoch\n",
      "Test set accuracy:\n",
      "0.9333333333333333\n"
     ]
    }
   ],
   "source": [
    "split = StratifiedShuffleSplit(n_splits=3, test_size=0.2, random_state=42)\n",
    "for train_index, test_index in split.split(df, df[\"target\"]):\n",
    "    strat_train_set = df.loc[train_index]\n",
    "    strat_test_set = df.loc[test_index]\n",
    "    \n",
    "    \n",
    "    # Use strat_train_set and strat_test_set to train and evaluate your model\n",
    "    model = softmaxRegressor(lr=0.07, \n",
    "                             epochs=500, \n",
    "                             early_stopping=True)\n",
    "    \n",
    "    model.fit(strat_train_set.drop(columns=[\"target\"]),\n",
    "              strat_train_set[\"target\"],\n",
    "              batch_size=25)\n",
    "    \n",
    "    print(\"Test set accuracy:\")\n",
    "    print(model.score(strat_test_set.drop(columns=[\"target\"]),\n",
    "                      strat_test_set[\"target\"]))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
